{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd = pd.read_excel('ML_project_kidney_disease-1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>rbc</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "      <th>Unnamed: 30</th>\n",
       "      <th>Unnamed: 31</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>LEGEND</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "      <th>Unnamed: 37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Julian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>present</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
       "0   0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "1   1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "2   2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
       "3   3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
       "4   4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
       "\n",
       "   ...  Unnamed: 28  Unnamed: 29  Unnamed: 30  Unnamed: 31  Unnamed: 32  \\\n",
       "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 33   LEGEND Unnamed: 35 Unnamed: 36 Unnamed: 37  \n",
       "0          NaN   Julian         NaN         NaN         NaN  \n",
       "1          NaN    Peter         NaN         NaN         NaN  \n",
       "2          NaN   Rachel         NaN         NaN         NaN  \n",
       "3          NaN  Taylor          NaN         NaN         NaN  \n",
       "4          NaN      NaN         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if it loaded successfully\n",
    "df_ckd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 38 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              400 non-null    int64  \n",
      " 1   age             391 non-null    float64\n",
      " 2   bp              388 non-null    float64\n",
      " 3   sg              353 non-null    float64\n",
      " 4   al              354 non-null    float64\n",
      " 5   su              351 non-null    float64\n",
      " 6   rbc             248 non-null    object \n",
      " 7   pc              335 non-null    object \n",
      " 8   pcc             396 non-null    object \n",
      " 9   ba              396 non-null    object \n",
      " 10  bgr             356 non-null    float64\n",
      " 11  bu              381 non-null    float64\n",
      " 12  sc              383 non-null    float64\n",
      " 13  sod             313 non-null    float64\n",
      " 14  pot             312 non-null    float64\n",
      " 15  hemo            348 non-null    float64\n",
      " 16  pcv             330 non-null    object \n",
      " 17  wc              295 non-null    object \n",
      " 18  rc              270 non-null    object \n",
      " 19  htn             398 non-null    object \n",
      " 20  dm              398 non-null    object \n",
      " 21  cad             398 non-null    object \n",
      " 22  appet           399 non-null    object \n",
      " 23  pe              399 non-null    object \n",
      " 24  ane             399 non-null    object \n",
      " 25  classification  400 non-null    object \n",
      " 26  CKD_class       400 non-null    int64  \n",
      " 27  Unnamed: 27     0 non-null      float64\n",
      " 28  Unnamed: 28     0 non-null      float64\n",
      " 29  Unnamed: 29     0 non-null      float64\n",
      " 30  Unnamed: 30     0 non-null      float64\n",
      " 31  Unnamed: 31     0 non-null      float64\n",
      " 32  Unnamed: 32     0 non-null      float64\n",
      " 33  Unnamed: 33     0 non-null      float64\n",
      " 34  LEGEND          4 non-null      object \n",
      " 35  Unnamed: 35     0 non-null      float64\n",
      " 36  Unnamed: 36     0 non-null      float64\n",
      " 37  Unnamed: 37     0 non-null      float64\n",
      "dtypes: float64(21), int64(2), object(15)\n",
      "memory usage: 118.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# looking at the datastructure\n",
    "# looking for any undesired columns\n",
    "df_ckd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the desired columns: \n",
    "df_ckd = df_ckd[[\"id\",\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"classification\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              400 non-null    int64  \n",
      " 1   age             391 non-null    float64\n",
      " 2   bp              388 non-null    float64\n",
      " 3   sg              353 non-null    float64\n",
      " 4   al              354 non-null    float64\n",
      " 5   su              351 non-null    float64\n",
      " 6   rbc             248 non-null    object \n",
      " 7   pc              335 non-null    object \n",
      " 8   pcc             396 non-null    object \n",
      " 9   ba              396 non-null    object \n",
      " 10  bgr             356 non-null    float64\n",
      " 11  bu              381 non-null    float64\n",
      " 12  sc              383 non-null    float64\n",
      " 13  sod             313 non-null    float64\n",
      " 14  pot             312 non-null    float64\n",
      " 15  hemo            348 non-null    float64\n",
      " 16  pcv             330 non-null    object \n",
      " 17  wc              295 non-null    object \n",
      " 18  rc              270 non-null    object \n",
      " 19  htn             398 non-null    object \n",
      " 20  dm              398 non-null    object \n",
      " 21  cad             398 non-null    object \n",
      " 22  appet           399 non-null    object \n",
      " 23  pe              399 non-null    object \n",
      " 24  ane             399 non-null    object \n",
      " 25  classification  400 non-null    object \n",
      "dtypes: float64(11), int64(1), object(14)\n",
      "memory usage: 81.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "age                 9\n",
       "bp                 12\n",
       "sg                 47\n",
       "al                 46\n",
       "su                 49\n",
       "rbc               152\n",
       "pc                 65\n",
       "pcc                 4\n",
       "ba                  4\n",
       "bgr                44\n",
       "bu                 19\n",
       "sc                 17\n",
       "sod                87\n",
       "pot                88\n",
       "hemo               52\n",
       "pcv                70\n",
       "wc                105\n",
       "rc                130\n",
       "htn                 2\n",
       "dm                  2\n",
       "cad                 2\n",
       "appet               1\n",
       "pe                  1\n",
       "ane                 1\n",
       "classification      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the datastructure and keep the desired columns\n",
    "df_ckd.info()\n",
    "# Check for missing data\n",
    "df_ckd.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of continous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ckd_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-627dd83c6939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ckd_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ckd_1' is not defined"
     ]
    }
   ],
   "source": [
    "y1 = df_ckd_1.age\n",
    "y1 = y1.dropna()\n",
    "plt.hist(y1, bins=np.arange(1, 100,5), alpha=0.5, label=\"y1\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1['ane'].value_counts().plot(kind='bar')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Ane\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1['classification'].value_counts().plot(kind='bar')\n",
    "plt.legend()\n",
    "plt.xlabel(\"classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification variable \n",
    "df_ckd_1[\"classification\"] = df_ckd_1[\"classification\"].astype(str)\n",
    "clas = df_ckd_1['classification']\n",
    "print(clas.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_clas = (clas.isnull().sum()/len(clas)) * 100\n",
    "print(\"missing clas: \", missing_clas, \"%\" )\n",
    "\n",
    "print(df_ckd_1['pc'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HANDLING MISSING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows has at least one missing values \n",
    "sum(df_ckd.apply(lambda x: sum(x.isnull().values), axis = 1)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the row entry that has more than 30% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualting the percentage of missing data in each row\n",
    "percent_missing = df_ckd.apply(lambda x: sum(x.isnull().values), axis = 1) / len(df_ckd.columns) * 100\n",
    "df_ckd['percent_missing']=percent_missing\n",
    "df_ckd.head()\n",
    "#df_ckd.head()\n",
    "# Removing the column that has more than 30\n",
    "\n",
    "percent_cutoff = 30\n",
    "df_ckd_1 = df_ckd[df_ckd.percent_missing < percent_cutoff]\n",
    "\n",
    "\n",
    "df_ckd_1.info()\n",
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed 31 observations for the rows that has more than 30% missing.\n",
    "\n",
    "Next, we want to know how many missing for each column and percentage of missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['count']=df_ckd_1.isnull().sum()\n",
    "missing_values['percentage']=df_ckd_1.isnull().sum()/len(df_ckd_1.classification)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.drop('percent_missing',axis = 0)\n",
    "missing_values.sort_values(by=\"percentage\",axis = 0, ascending = True).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the indices for better analyzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at each variable importance to the predictor. \n",
    "\n",
    "T-test is used on continous variable .\n",
    "\n",
    "Chi-sqaure is used on categorical variable.\n",
    "\n",
    "Output should be: Pot (Potassium is not significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############CODE NEEDED FOR T-test and CHI-SQAURE#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bioinfokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import math\n",
    "from bioinfokit.analys import get_data, stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Age T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"age\", evar=False)\n",
    "print('\\033[1m' + 'Blood Pressure T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bp\", evar=False)\n",
    "\n",
    "print('\\033[1m' + 'Blood Glucose Random T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bgr\", evar=False)\n",
    "print('\\033[1m' + 'Blood Urea T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bu\", evar=False)\n",
    "print('\\033[1m' + 'Serum Creatinine T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"sc\", evar=False)\n",
    "print('\\033[1m' + 'Sodium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"sod\", evar=False)\n",
    "print('\\033[1m' + 'Potassium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pot\", evar=False)\n",
    "print('\\033[1m' + 'Hemoglobin T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"hemo\", evar=False)\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'Packed Cell Volume T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pcv\", evar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_2 = df_ckd_1\n",
    "df_ckd_2[\"wc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_2[\"rc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_2[['wc']]=df_ckd_2[['wc']].astype(float)\n",
    "df_ckd_2[['rc']]=df_ckd_2[['rc']].astype(float)\n",
    "\n",
    "\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'White Blood Cell Count T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_2, xfac=\"classification\", res=\"wc\", evar=False)\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'Red Blood Cell Count T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_2, xfac=\"classification\", res=\"rc\", evar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look on Potassium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Potassium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pot\", evar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df_ckd_1.pot[df_ckd_1['classification'] == 'ckd']\n",
    "y1 = y1.dropna()\n",
    "y2 = df_ckd_1.pot[df_ckd_1['classification'] == 'notckd']\n",
    "y2 = y2.dropna()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(y1, bins=np.arange(1, 10, 1), alpha=0.5, label=\"y1\")\n",
    "plt.hist(y2, bins=np.arange(1, 10, 1), alpha=0.5, label=\"y2\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Pot\")\n",
    "plt.ylabel(\"Cases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Potassium since the t-test showed non-signifcant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.replace(\"ckd\\t\",\"ckd\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['wc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rbc\")\n",
    "df_table_rbc = pd.crosstab(df_ckd_1['classification'],df_ckd_1['rbc'])\n",
    "#print(df_table_rbc)\n",
    "df_table_rbc.values\n",
    "Observed_values_rbc = df_table_rbc.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values_rbc)\n",
    "val_rbc=stats.chi2_contingency(df_table_rbc)\n",
    "val_rbc\n",
    "expected_values_rbc=val_rbc[3]\n",
    "no_of_rows_rbc=len(df_table_rbc.iloc[0:2,0])\n",
    "no_of_columns_rbc=len(df_table_rbc.iloc[0,0:2])\n",
    "ddof_rbc=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "#print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "p_value_rbc=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value_rbc)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value_rbc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcc\n",
    "print(\"pcc\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pcc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC\n",
    "print(\"PC\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTN\n",
    "print(\"HTN\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cad\n",
    "print(\"cad\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM\n",
    "print(\"DM\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPET\n",
    "print(\"APPET\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PE\n",
    "print(\"PE\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANE\n",
    "print(\"ANE\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BA\n",
    "\n",
    "print(\"BA\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing the variables that has less than 12% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appet\n",
    "\n",
    "#calculate the amount of missing data\n",
    "appet = df_ckd_1['appet']\n",
    "missing_appet = (appet.isnull().sum()/len(appet)) * 100\n",
    "print(\"missing appet: \", missing_appet, \"%\" )\n",
    "\n",
    "\n",
    "\n",
    "print(df_ckd_1['appet'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"appet\"].replace(np.nan, \"good\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ane\n",
    "\n",
    "#calculate the amount of missing data\n",
    "ane = df_ckd_1['ane']\n",
    "ane.describe()\n",
    "missing_ane = (ane.isnull().sum()/len(ane)) * 100\n",
    "print(\"missing ane: \", missing_ane, \"%\" )\n",
    "\n",
    "#######################################\n",
    "\n",
    "print(df_ckd_1['ane'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"ane\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe\n",
    "\n",
    "#calculate the amount of missing data\n",
    "pe = df_ckd_1['pe']\n",
    "missing_pe = (pe.isnull().sum()/len(pe)) * 100\n",
    "print(\"missing pe: \", missing_pe, \"%\" )\n",
    "\n",
    "\n",
    "\n",
    "print(df_ckd_1['pe'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"pe\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cad\n",
    "\n",
    "#calculate the amount of missing data\n",
    "cad = df_ckd_1['cad']\n",
    "missing_cad = (cad.isnull().sum()/len(cad)) * 100\n",
    "print(\"missing cad: \", missing_cad, \"%\" )\n",
    "\n",
    "\n",
    "df_ckd_1[\"cad\"].replace(\"\\tno\", \"no\", inplace=True)\n",
    "\n",
    "print(df_ckd_1['cad'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"cad\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#htn\n",
    "\n",
    "#calculate the amount of missing data\n",
    "htn = df_ckd_1['htn']\n",
    "missing_htn = (htn.isnull().sum()/len(htn)) * 100\n",
    "print(\"missing htn: \", missing_htn, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['htn'].value_counts())\n",
    "\n",
    "#Replace the \"htn\" with the most frequent density\n",
    "df_ckd_1[\"htn\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm\n",
    "\n",
    "#calculate the amount of missing data\n",
    "dm = df_ckd_1['dm']\n",
    "missing_dm = (dm.isnull().sum()/len(dm)) * 100\n",
    "print(\"missing dm: \", missing_dm, \"%\" )\n",
    "\n",
    "\n",
    "#Fixing the space issue in our dataframe\n",
    "df_ckd_1[\"dm\"].replace(\"\\tno\", \"no\", inplace=True)\n",
    "df_ckd_1[\"dm\"].replace(\"\\tyes\", \"yes\", inplace=True)\n",
    "\n",
    "print(df_ckd_1['dm'].value_counts())\n",
    "#Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"dm\"].replace(np.nan, \"no\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcc\n",
    "#calculate the amount of missing data\n",
    "pcc = df_ckd_1['pcc']\n",
    "missing_pcc = (pcc.isnull().sum()/len(pcc)) * 100\n",
    "print(\"missing pcc: \", missing_pcc, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['pcc'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"pcc\"].replace(np.nan, \"notpresent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ba\n",
    "ba = df_ckd_1['ba']\n",
    "missing_ba = (ba.isnull().sum()/len(ba)) * 100\n",
    "print(\"missing ba: \", missing_ba, \"%\" )\n",
    "\n",
    "print(df_ckd_1['ba'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"ba\"].replace(np.nan, \"notpresent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "\n",
    "age = df_ckd_1['age']\n",
    "print(age.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_age = (age.isnull().sum()/len(age)) * 100\n",
    "number_of_missing_data=age.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "print(\"missing age: \", missing_age, \"%\" )\n",
    "sns.distplot(age)\n",
    "\n",
    "# Adding in the missing values with average for age\n",
    "\n",
    "avg_age=df_ckd_1[\"age\"].mean(axis=0)\n",
    "print(\"Average of age\", avg_age)\n",
    "df_ckd_1[\"age\"].replace(np.nan, avg_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc\n",
    "sc = df_ckd_1['sc']\n",
    "print(sc.describe()) #calculate the amount of missing data\n",
    "missing_sc = (sc.isnull().sum()/len(sc)) * 100\n",
    "print(\"missing sc: \", missing_sc, \"%\" )\n",
    "sns.distplot(sc)\n",
    "\n",
    "avg_sc=df_ckd_1[\"sc\"].mean(axis=0)\n",
    "print(\"Average of sc\", avg_sc)\n",
    "df_ckd_1[\"sc\"].replace(np.nan, avg_sc, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bp\n",
    "# it is a continous variable. May look categorical\n",
    "bp = df_ckd_1['bp']\n",
    "print(bp.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_bp = (bp.isnull().sum()/len(bp)) * 100\n",
    "print(\"missing bp: \", missing_bp, \"%\" )\n",
    "sns.distplot(bp)\n",
    "\n",
    "# Updating the missing values\n",
    "avg_bp=df_ckd_1[\"bp\"].mean(axis=0)\n",
    "print(\"Average of bp\", avg_bp)\n",
    "df_ckd_1[\"bp\"].replace(np.nan, avg_bp, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bu \n",
    "bu = df_ckd_1['bu']\n",
    "print(bu.describe()) #calculate the amount of missing data\n",
    "missing_bu = (bu.isnull().sum()/len(bu)) * 100\n",
    "print(\"missing bu: \", missing_bu, \"%\" )\n",
    "sns.distplot(bu)\n",
    "\n",
    "# Updating the bu\n",
    "avg_bu=df_ckd_1[\"bu\"].mean(axis=0)\n",
    "print(\"Average of bu\", avg_bu)\n",
    "df_ckd_1[\"bu\"].replace(np.nan, avg_bu, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# al\n",
    "al = df_ckd_1['al']\n",
    "print(al.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_al = (al.isnull().sum()/len(al)) * 100\n",
    "print(\"missing rbc: \", missing_al, \"%\" )\n",
    "al_group = df_ckd.groupby('su').id.agg(['count'])\n",
    "print(al_group)\n",
    "\n",
    "# Replace the \"al\" with the most frequent density\n",
    "df_ckd_1[\"al\"].replace(np.nan, \"0.0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg\n",
    "# Even though it should be continous. it is categorical\n",
    "sg = df_ckd_1['sg']\n",
    "print(sg.describe()) #calculate the amount of missing data\n",
    "missing_sg = (sg.isnull().sum()/len(sg)) * 100\n",
    "print(\"missing sg: \", missing_sg, \"%\" )\n",
    "print(df_ckd_1['sg'].value_counts())\n",
    "\n",
    "# Replace the \"sg\" with the most frequent density\n",
    "df_ckd_1[\"sg\"].replace(np.nan, \"1.020\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# su variable \n",
    "# It is a categorical variable\n",
    "# Convert that into an object\n",
    "df_ckd_1[\"su\"].astype(object)\n",
    "\n",
    "su = df_ckd_1['su']\n",
    "print(su.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_su = (su.isnull().sum()/len(su)) * 100\n",
    "print(\"missing su: \", missing_al, \"%\" )\n",
    "\n",
    "#Finding the most frequent\n",
    "su_group = df_ckd.groupby('su').id.agg(['count'])\n",
    "print(su_group)\n",
    "\n",
    "\n",
    "# Replace the \"al\" with the most frequent density\n",
    "df_ckd_1[\"su\"].replace(np.nan, \"0.0\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hemo\n",
    "hemo = df_ckd_1['hemo']\n",
    "print(hemo.describe()) #calculate the amount of missing data\n",
    "missing_hemo = (hemo.isnull().sum()/len(hemo)) * 100\n",
    "print(\"missing hemo: \", missing_hemo, \"%\" )\n",
    "number_of_missing_data=hemo.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "sns.distplot(hemo)\n",
    "\n",
    "avg_hemo=df_ckd_1[\"hemo\"].mean(axis=0)\n",
    "print(\"Average of hemo\", avg_hemo)\n",
    "df_ckd_1[\"hemo\"].replace(np.nan, avg_hemo, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgr\n",
    "bgr = df_ckd_1['bgr']\n",
    "print(bgr.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_bgr = (bgr.isnull().sum()/len(bgr)) * 100\n",
    "print(\"missing ba: \", missing_bgr, \"%\" )\n",
    "sns.distplot(bgr)\n",
    "avg_bgr=df_ckd_1[\"bgr\"].mean(axis=0)\n",
    "print(\"Average of bgr\", avg_bgr)\n",
    "df_ckd_1[\"bgr\"].replace(np.nan, avg_bgr, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCV\n",
    "\n",
    "df_ckd_1[\"pcv\"].replace(\"\\t43\", \"43\", inplace=True)\n",
    "df_ckd_1[\"pcv\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_1['pcv'].value_counts()\n",
    "\n",
    "df_ckd_1[\"pcv\"] = df_ckd_1[\"pcv\"].astype(float)\n",
    "pcv = df_ckd_1['pcv']\n",
    "print(pcv.describe()) #calculate the amount of missing data\n",
    "missing_pcv = (pcv.isnull().sum()/len(pcv)) * 100\n",
    "print(\"missing pcv: \", missing_pcv, \"%\" )\n",
    "sns.distplot(pcv)\n",
    "\n",
    "\n",
    "avg_pcv=df_ckd_1[\"pcv\"].mean(axis=0)\n",
    "print(\"Average of pcv\", avg_pcv)\n",
    "# Replace the \"pcv\" with the mean \n",
    "df_ckd_1[\"pcv\"].replace(np.nan, avg_pcv, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc variable \n",
    "\n",
    "pc = df_ckd_1['pc']\n",
    "print(pc.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_pc = (pc.isnull().sum()/len(pc)) * 100\n",
    "print(\"missing pc: \", missing_al, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['pc'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"pc\"].replace(np.nan, \"normal\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Reviewing the current changes made ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()\n",
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################### Now handling the more complicated issue####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the sod\n",
    "\n",
    "* The current method is replaced with average.\n",
    "* correlated bp and cad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Data ready for the Linear Regression\n",
    "data = df_ckd_1[['id','bp','cad','sod']]\n",
    "# changing the cad\n",
    "data = pd.get_dummies(data, columns = ['cad'])\n",
    "del data['cad_no']\n",
    "missing_data = data['sod'].isnull()\n",
    "missing_data_tip = pd.DataFrame(data[['bp','cad_yes']][missing_data])\n",
    "data_nomissing = data.dropna()\n",
    "x = data_nomissing[['bp','cad_yes']]\n",
    "y = data_nomissing['sod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "# Descriptive statistic for evaluating the model\n",
    "yhat = lm.predict(X_test)\n",
    "SS_R = sum((yhat-np.mean(y_test))**2)       \n",
    "SS_Total = sum((y_test-np.mean(y_test))**2) \n",
    "r_squared = (float(SS_R))/SS_Total\n",
    "mse = mean_squared_error(y_test, yhat)\n",
    "\n",
    "print(\"r_sqaured: \",round(r_squared,2))\n",
    "print(\"mse: \", round(mse,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing on to the orginal dataframe\n",
    "data_pred = lm.predict(missing_data_tip)\n",
    "missing_data_tip['data_pred'] = data_pred\n",
    "data[\"data_pred\"] = missing_data_tip[\"data_pred\"]\n",
    "data[\"sod\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"data_pred\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"sod\"] = data[\"sod\"]+data[\"data_pred\"]\n",
    "df_ckd_1[\"sod\"]=data[\"sod\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing with the mean average (NOT USED ANYMORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sod \n",
    "\n",
    "# sod = df_ckd_1['sod']\n",
    "# print(sod.describe()) #calculate the amount of missing data\n",
    "# missing_sod = (sod.isnull().sum()/len(sod)) * 100\n",
    "# print(\"missing sod: \", missing_sod, \"%\" )\n",
    "# sns.distplot(sod)\n",
    "\n",
    "# # Changing sod\n",
    "# avg_sod=df_ckd_1[\"sod\"].mean(axis=0)\n",
    "# print(\"Average of sod\", avg_sod)\n",
    "# df_ckd_1[\"sod\"].replace(np.nan, avg_sod, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WC\n",
    "\n",
    "* We found an article that htn has a high correlation with the wc. \n",
    "* htn: yes has an average of 8094\n",
    "* htn: no has an average 8988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ckd_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-510350863daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# wc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_ckd_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ckd_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmissing_wc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ckd_1' is not defined"
     ]
    }
   ],
   "source": [
    "# wc\n",
    "\n",
    "df_ckd_1[\"wc\"].describe\n",
    "wc = df_ckd_1['wc']\n",
    "missing_wc = (wc.isnull().sum()/len(wc)) * 100\n",
    "print(\"missing wc: \", missing_wc, \"%\" )\n",
    "wc.describe()\n",
    "\n",
    "# Correct the wc from missing data and into float\n",
    "df_ckd_1[\"wc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_1[['wc']]=df_ckd_1[['wc']].astype(float)\n",
    "wc.describe()\n",
    "\n",
    "# Need to replace the missing values for wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.groupby('htn').wc.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above results, we need to replace 8094 for htn is no , and replace 8988 for htn is yes\n",
    "for i in range(len(df_ckd_1['wc'])):\n",
    "    if pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]) == True and df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('htn')] == 'yes':\n",
    "        df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]= 8988\n",
    "        print(i)\n",
    "    elif pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]) == True and df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('htn')] == 'no':\n",
    "        df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]= 8094\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dealing with RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC\n",
    "corrMatrix = df_ckd_1.corr()\n",
    "print (corrMatrix)\n",
    "\n",
    "# From the correlatoin matrix. PCV and RC are highly correlated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Data ready for the Linear Regression\n",
    "data = df_ckd_1[['id','rc','hemo']]\n",
    "\n",
    "missing_data = data['rc'].isnull()\n",
    "missing_data_tip = pd.DataFrame(data[['hemo']][missing_data])\n",
    "data_nomissing = data.dropna()\n",
    "x = data_nomissing[['hemo']]\n",
    "y = data_nomissing['rc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "# Descriptive statistic for evaluating the model\n",
    "yhat = lm.predict(X_test)\n",
    "SS_R = sum((yhat-np.mean(y_test))**2)       \n",
    "SS_Total = sum((y_test-np.mean(y_test))**2) \n",
    "r_squared = (float(SS_R))/SS_Total\n",
    "mse = mean_squared_error(y_test, yhat)\n",
    "\n",
    "print(\"r_sqaured: \",round(r_squared,2))\n",
    "print(\"mse: \", round(mse,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing on to the orginal dataframe\n",
    "data_pred = lm.predict(missing_data_tip)\n",
    "missing_data_tip['data_pred'] = data_pred\n",
    "data[\"data_pred\"] = missing_data_tip[\"data_pred\"]\n",
    "data[\"rc\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"data_pred\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"rc\"] = data[\"rc\"]+data[\"data_pred\"]\n",
    "df_ckd_1[\"rc\"]=data[\"rc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code below no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC \n",
    "\n",
    "# df_ckd_1[\"rc\"].astype(float)\n",
    "\n",
    "# #calculate the amount of missing data\n",
    "# rc = df_ckd_1['rc']\n",
    "# missing_rc = (rc.isnull().sum()/len(rc)) * 100\n",
    "# print(\"missing rc: \", missing_rc, \"%\" )\n",
    "# print(df_ckd_1['rc'].describe())\n",
    "\n",
    "# # Replace it with mean \n",
    "# avg_rc=df_ckd_1[\"rc\"].mean(axis=0)\n",
    "# print(\"Average of rc\", avg_rc)\n",
    "# df_ckd_1[\"rc\"].replace(np.nan, avg_rc, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focus on RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBC\n",
    "\n",
    "#calculate the amount of missing data\n",
    "rbc = df_ckd_1['rbc']\n",
    "missing_rbc = (rbc.isnull().sum()/len(rbc)) * 100\n",
    "print(\"missing rbc: \", missing_rbc, \"%\" )\n",
    "\n",
    "number_of_missing_data=rbc.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "\n",
    "print(df_ckd_1['rbc'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_ckd_1)):\n",
    "    if pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('rbc')]) == True and df_ckd_1.iloc[i]['rc'] > 3.92 and df_ckd_1.iloc[i]['rc'] < 5.65:\n",
    "        df_ckd_1.iloc[i, df_ckd_1.columns.get_loc('rbc')] = 'normal'\n",
    "        \n",
    "    elif pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('rbc')]) == True:\n",
    "        df_ckd_1.iloc[i, df_ckd_1.columns.get_loc('rbc')] = 'abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the update_ckd file\n",
    "df_ckd_1.to_csv(\"df_ckd_1.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers for the continous variable\n",
    "It deleted too many variables. So this section was not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 = df_ckd_1.quantile(0.25)\n",
    "# Q3 = df_ckd_1.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ckd_2 = df_ckd_1[~((df_ckd_1 < (Q1 - 1.5 * IQR)) |(df_ckd_1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# df_ckd_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ckd_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Code the categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the sg\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['sg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the al\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['al'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the su\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['su'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the rbc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['rbc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pcc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the ba\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['ba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the htn\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['htn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the dm\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['dm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the cad\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['cad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the appet\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['appet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pe\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the ane\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['ane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the classification\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping unused column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing percent_missing\n",
    "del df_ckd_1['percent_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the categorical n-1 variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['sg_1.005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['al_0.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['su_0.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['rbc_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pc_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pcc_notpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['ba_notpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['htn_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['dm_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['cad_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['appet_poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pe_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['ane_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['classification_notckd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.to_csv(\"df_ckd_1.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "* Rename the variable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.rename(columns={'rbc_abnormal':'rbc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'ba_present':'ba'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pc_abnormal':'pc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pcc_present':'pcc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'htn_yes':'htn'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'dm_yes':'dm'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'cad_yes':'cad'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'appet_good':'appet'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pe_yes':'pe'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'ane_yes':'ane'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'classification_ckd':'classification'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################\n",
    "* Complete data wrangling\n",
    "* Next, set training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for predictors\n",
    "df_ckd_predictors = df_ckd_1.loc[:, df_ckd_1.columns != 'classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for response\n",
    "df_ckd_response = df_ckd_1['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ckd_predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5e6919a7fc04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 60% trainning and 40% testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ckd_predictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ckd_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ckd_predictors' is not defined"
     ]
    }
   ],
   "source": [
    "# 60% trainning and 40% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ckd_predictors, df_ckd_response, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"X_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"y_train.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(\"y_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################\n",
    "Model Comparison ######################################\n",
    "1. Logistic: Taylor, Rachel \n",
    "2. Nearest Neighbor: Peter, Juilanne \n",
    "3. Random Forest: Gabby, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the performance of model\n",
    "1) Beat the naive rule \n",
    "2) validation hitrate and trainning hitrate within 3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_count = y_test.sum()\n",
    "naive_length = len(y_test)\n",
    "\n",
    "Naive_rule = naive_count/naive_length * 100\n",
    "print(\"Naive Rule: \", round(Naive_rule, 2) ,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for measuring our model performance used in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate: just pass in the confusion matrix of either validation data or training data\n",
    "def calcHR(conf):\n",
    "    hey=conf[0,0]\n",
    "    ho=conf[1,1]\n",
    "    result=(hey+ho)/conf.sum()*100\n",
    "    return result\n",
    "# False Positive: input confusion matrix to calculate the false positives in the data set. return a %\n",
    "def calcFP(conf):\n",
    "    result=(conf[0,1].astype('float'))/(conf[0,1].astype('float')+conf[1,1].astype('float')) * 100\n",
    "    return result\n",
    "# False Negative: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcFN(conf):\n",
    "    result=(conf[1,0].astype('float'))/(conf[0,0].astype('float')+conf[1,0].astype('float')) * 100\n",
    "    return result\n",
    "# Sensitivity: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcSens(conf):\n",
    "    result=(conf[1,1].astype('float'))/(conf[1,0].astype('float')+conf[1,1].astype('float')) * 100\n",
    "    return result\n",
    "# Specificity: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcSpec(conf):\n",
    "    result=(conf[0,0].astype('float'))/(conf[0,0].astype('float')+conf[0,1].astype('float')) * 100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do List\n",
    "\n",
    "\n",
    "1) ROC Curve\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this website as reference: \n",
    "https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K = 12\n",
    "# Classifier with Euclidean distance for determining the proximity between neighboring points\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=12, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_validate = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "confValid = confusion_matrix(y_test, y_pred_validate)\n",
    "confTrain = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class) or the Naive rule\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = knn.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('Naive Rule: ROC AUC=%.3f' % (ns_auc))\n",
    "print('NNA: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Nearest Neighbor')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying different nearest neighbor number from 1 to 9\n",
    "\n",
    "\n",
    "Writing the for loop through different k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "test_hitrate = np.empty(len(dep))\n",
    "test_false_pos = np.empty(len(dep))\n",
    "test_false_neg = np.empty(len(dep))\n",
    "test_sens = np.empty(len(dep))\n",
    "test_spec = np.empty(len(dep))\n",
    "test_one_minus_spec = np.empty(len(dep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "    conf_test = confusion_matrix(y_test, y_pred)\n",
    "    test_hitrate[i] = (conf_test[0,0].astype('float')+conf_test[1,1].astype('float'))/conf_test.sum().astype('float') * 100\n",
    "    test_false_pos[i] = (conf_test[0,1].astype('float'))/(conf_test[0,1].astype('float')+conf_test[1,1].astype('float')) * 100\n",
    "    test_false_neg[i] = (conf_test[1,0].astype('float'))/(conf_test[0,0].astype('float')+conf_test[1,0].astype('float')) * 100\n",
    "    test_sens[i] = (conf_test[1,1].astype('float'))/(conf_test[1,0].astype('float')+conf_test[1,1].astype('float')) * 100\n",
    "    test_spec[i] = (conf_test[0,0].astype('float'))/(conf_test[0,0].astype('float')+conf_test[0,1].astype('float')) * 100\n",
    "    test_one_minus_spec[i] = 100-spec\n",
    "\n",
    "# Generate plot\n",
    "plt.title('Different k')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Different k values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Create a function to create a list\n",
    "def createList(r1, r2): \n",
    "    return [item for item in range(r1, r2+1)] \n",
    "      \n",
    "# testing it out:\n",
    "r1, r2 = -1, 1\n",
    "print(createList(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2c3e78739a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNNA_hitrate_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Check_for_vaild'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNNA_hitrate_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreateList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "NNA_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "NNA_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "NNA_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "NNA_hitrate_summary['Check_for_vaild'] = NNA_hitrate_summary['train_accuracy']-NNA_hitrate_summary['test_accuracy']\n",
    "NNA_hitrate_summary['k_value']=createList(1, 19)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = NNA_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "NNA_hitrate_summary = NNA_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################Logistic Regressoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to look into forward selection \n",
    "\n",
    "\n",
    "Need to look into backward selection \n",
    "\n",
    "Need to get odds-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_validate1=logreg.predict(X_test)\n",
    "y_pred_train1 = logreg.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid1 = confusion_matrix(y_test, y_pred_validate1)\n",
    "confTrain1 = confusion_matrix(y_train, y_pred_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain1),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid1),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid1),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid1),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Model's ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Logistic: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.exp(logreg.intercept_).astype('float')\n",
    "model_odds = pd.DataFrame(df_ckd_predictors.columns)\n",
    "model_odds['odds-ratio'] = pd.DataFrame(np.exp(logreg.coef_ * 1 + logreg.intercept_)).transpose()/base\n",
    "model_odds.rename(columns={0:'predictors'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, bootstrap=True)\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "y_pred_validate2=model.predict(X_test)\n",
    "y_pred_train2 = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns),\n",
    "'importance': model.feature_importances_}).\\\n",
    "sort_values('importance', ascending = False)\n",
    "fi.head()\n",
    "\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('feature','importance','barh', figsize=(12,7), legend=False)\n",
    "\n",
    "# fi[:15] label the number of variable cutoff\n",
    "plot_fi(fi[:15]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid2 = confusion_matrix(y_test, y_pred_validate2)\n",
    "confTrain2 = confusion_matrix(y_train, y_pred_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confValid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain2),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid2),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid2),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid2),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = model.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Random Forest: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the Randomforest\n",
    "Varying the branches of the randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "\n",
    "for i, k in enumerate(dep):\n",
    "    clf = RandomForestClassifier(max_depth=k)\n",
    "\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    train_accuracy[i] = clf.score(X_train, y_train)\n",
    "\n",
    "    test_accuracy[i] = clf.score(X_test, y_test)\n",
    "\n",
    "plt.title('clf: Varying depth of forest branch')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "RF_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "RF_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "RF_hitrate_summary['Check_for_vaild'] = RF_hitrate_summary['train_accuracy']-RF_hitrate_summary['test_accuracy']\n",
    "RF_hitrate_summary['Depth of tree']=createList(1, 19)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = RF_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "RF_hitrate_summary = RF_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hitrate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################\n",
    "Ideas for some improvement on NNA and Logistic\n",
    "* Use the important variable identified from Random Forest\n",
    "* hemo, pcv, sc, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_response = df_ckd_1['classification']\n",
    "features=['hemo','pcv','sc','rc']\n",
    "df_ckd_predictors = df_ckd_1[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ckd_predictors, df_ckd_response, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "test_hitrate = np.empty(len(dep))\n",
    "test_false_pos = np.empty(len(dep))\n",
    "test_false_neg = np.empty(len(dep))\n",
    "test_sens = np.empty(len(dep))\n",
    "test_spec = np.empty(len(dep))\n",
    "test_one_minus_spec = np.empty(len(dep))\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "    conf_test = confusion_matrix(y_test, y_pred)\n",
    "    test_hitrate[i] = (conf_test[0,0].astype('float')+conf_test[1,1].astype('float'))/conf_test.sum().astype('float') * 100\n",
    "    test_false_pos[i] = (conf_test[0,1].astype('float'))/(conf_test[0,1].astype('float')+conf_test[1,1].astype('float')) * 100\n",
    "    test_false_neg[i] = (conf_test[1,0].astype('float'))/(conf_test[0,0].astype('float')+conf_test[1,0].astype('float')) * 100\n",
    "    test_sens[i] = (conf_test[1,1].astype('float'))/(conf_test[1,0].astype('float')+conf_test[1,1].astype('float')) * 100\n",
    "    test_spec[i] = (conf_test[0,0].astype('float'))/(conf_test[0,0].astype('float')+conf_test[0,1].astype('float')) * 100\n",
    "    test_one_minus_spec[i] = 100-spec\n",
    "\n",
    "# Generate plot\n",
    "plt.title('Different k')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Different k values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "NNA_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "NNA_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "NNA_hitrate_summary['Check_for_vaild'] = NNA_hitrate_summary['train_accuracy']-NNA_hitrate_summary['test_accuracy']\n",
    "NNA_hitrate_summary['k_value']=createList(1, 19)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = NNA_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "NNA_hitrate_summary = NNA_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K = 4\n",
    "# Classifier with Euclidean distance for determining the proximity between neighboring points\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_validate = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "confValid = confusion_matrix(y_test, y_pred_validate)\n",
    "confTrain = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class) or the Naive rule\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = knn.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('Naive Rule: ROC AUC=%.3f' % (ns_auc))\n",
    "print('NNA: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Nearest Neighbor')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_validate1=logreg.predict(X_test)\n",
    "y_pred_train1 = logreg.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid4 = confusion_matrix(y_test, y_pred_validate1)\n",
    "confTrain4 = confusion_matrix(y_train, y_pred_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain4),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid4),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid4),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid4),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Logistic: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.exp(logreg.intercept_).astype('float')\n",
    "model_odds = pd.DataFrame(df_ckd_predictors.columns)\n",
    "model_odds['odds-ratio'] = pd.DataFrame(np.exp(logreg.coef_ * 1 + logreg.intercept_)).transpose()/base\n",
    "model_odds.rename(columns={0:'predictors'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
