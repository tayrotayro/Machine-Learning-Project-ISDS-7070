{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ML_project_kidney_disease.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-62b61d65a2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ckd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ML_project_kidney_disease.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ML_project_kidney_disease.xlsx'"
     ]
    }
   ],
   "source": [
    "df_ckd = pd.read_excel('ML_project_kidney_disease.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if it loaded successfully\n",
    "df_ckd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the datastructure\n",
    "# looking for any undesired columns\n",
    "df_ckd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the desired columns: \n",
    "df_ckd = df_ckd[[\"id\",\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"classification\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the datastructure and keep the desired columns\n",
    "df_ckd.info()\n",
    "# Check for missing data\n",
    "df_ckd.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd['ane'].value_counts().plot(kind='bar')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Ane\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd['classification'].value_counts().plot(kind='bar')\n",
    "plt.legend()\n",
    "plt.xlabel(\"classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification variable \n",
    "df_ckd[\"classification\"] = df_ckd[\"classification\"].astype(str)\n",
    "clas = df_ckd['classification']\n",
    "print(clas.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_clas = (clas.isnull().sum()/len(clas)) * 100\n",
    "print(\"missing clas: \", missing_clas, \"%\" )\n",
    "\n",
    "print(df_ckd['pc'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HANDLING MISSING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows has at least one missing values \n",
    "sum(df_ckd.apply(lambda x: sum(x.isnull().values), axis = 1)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the row entry that has more than 30% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualting the percentage of missing data in each row\n",
    "percent_missing = df_ckd.apply(lambda x: sum(x.isnull().values), axis = 1) / len(df_ckd.columns) * 100\n",
    "df_ckd['percent_missing']=percent_missing\n",
    "df_ckd.head()\n",
    "#df_ckd.head()\n",
    "# Removing the column that has more than 30\n",
    "\n",
    "percent_cutoff = 30\n",
    "df_ckd_1 = df_ckd[df_ckd.percent_missing < percent_cutoff]\n",
    "\n",
    "\n",
    "df_ckd_1.info()\n",
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed 31 observations for the rows that has more than 30% missing.\n",
    "\n",
    "Next, we want to know how many missing for each column and percentage of missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values['count']=df_ckd_1.isnull().sum()\n",
    "missing_values['percentage']=df_ckd_1.isnull().sum()/len(df_ckd_1.classification)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values.drop('percent_missing',axis = 0)\n",
    "missing_values.sort_values(by=\"percentage\",axis = 0, ascending = True).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the indices for better analyzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at each variable importance to the predictor. \n",
    "\n",
    "T-test is used on continous variable .\n",
    "\n",
    "Chi-sqaure is used on categorical variable.\n",
    "\n",
    "Output should be: Pot (Potassium is not significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############CODE NEEDED FOR T-test and CHI-SQAURE#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bioinfokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import math\n",
    "from bioinfokit.analys import get_data, stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Age T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"age\", evar=False)\n",
    "print('\\033[1m' + 'Blood Pressure T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bp\", evar=False)\n",
    "\n",
    "print('\\033[1m' + 'Blood Glucose Random T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bgr\", evar=False)\n",
    "print('\\033[1m' + 'Blood Urea T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"bu\", evar=False)\n",
    "print('\\033[1m' + 'Serum Creatinine T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"sc\", evar=False)\n",
    "print('\\033[1m' + 'Sodium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"sod\", evar=False)\n",
    "print('\\033[1m' + 'Potassium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pot\", evar=False)\n",
    "print('\\033[1m' + 'Hemoglobin T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"hemo\", evar=False)\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'Packed Cell Volume T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pcv\", evar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_2 = df_ckd_1\n",
    "df_ckd_2[\"wc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_2[\"rc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_2[['wc']]=df_ckd_2[['wc']].astype(float)\n",
    "df_ckd_2[['rc']]=df_ckd_2[['rc']].astype(float)\n",
    "\n",
    "\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'White Blood Cell Count T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_2, xfac=\"classification\", res=\"wc\", evar=False)\n",
    "print('\\033[1m \\033[91m \\033[4m' + 'Red Blood Cell Count T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_2, xfac=\"classification\", res=\"rc\", evar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look on Potassium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Potassium T Test:' + '\\033[0m')\n",
    "stat.ttsam(df=df_ckd_1, xfac=\"classification\", res=\"pot\", evar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df_ckd_1.pot[df_ckd_1['classification'] == 'ckd']\n",
    "y1 = y1.dropna()\n",
    "y2 = df_ckd_1.pot[df_ckd_1['classification'] == 'notckd']\n",
    "y2 = y2.dropna()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(y1, bins=np.arange(1, 10, 1), alpha=0.5, label=\"y1\")\n",
    "plt.hist(y2, bins=np.arange(1, 10, 1), alpha=0.5, label=\"y2\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Pot\")\n",
    "plt.ylabel(\"Cases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Potassium since the t-test showed non-signifcant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.replace(\"ckd\\t\",\"ckd\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['wc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rbc\")\n",
    "df_table_rbc = pd.crosstab(df_ckd_1['classification'],df_ckd_1['rbc'])\n",
    "#print(df_table_rbc)\n",
    "df_table_rbc.values\n",
    "Observed_values_rbc = df_table_rbc.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values_rbc)\n",
    "val_rbc=stats.chi2_contingency(df_table_rbc)\n",
    "val_rbc\n",
    "expected_values_rbc=val_rbc[3]\n",
    "no_of_rows_rbc=len(df_table_rbc.iloc[0:2,0])\n",
    "no_of_columns_rbc=len(df_table_rbc.iloc[0,0:2])\n",
    "ddof_rbc=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "#print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "p_value_rbc=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value_rbc)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value_rbc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcc\n",
    "print(\"pcc\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pcc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC\n",
    "print(\"PC\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTN\n",
    "print(\"HTN\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cad\n",
    "print(\"cad\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM\n",
    "print(\"DM\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPET\n",
    "print(\"APPET\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PE\n",
    "print(\"PE\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANE\n",
    "print(\"ANE\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BA\n",
    "\n",
    "print(\"BA\")\n",
    "df_table=pd.crosstab(df_ckd_1['classification'],df_ckd_1['pc'])\n",
    "# print(df_table)\n",
    "\n",
    "df_table.values\n",
    "Observed_values = df_table.values\n",
    "#print(\"Observed Values :-\\n\",Observed_values)\n",
    "\n",
    "val=stats.chi2_contingency(df_table)\n",
    "val\n",
    "\n",
    "expected_values=val[3]\n",
    "\n",
    "no_of_rows=len(df_table.iloc[0:2,0])\n",
    "no_of_columns=len(df_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "# print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing the variables that has less than 12% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appet\n",
    "\n",
    "#calculate the amount of missing data\n",
    "appet = df_ckd_1['appet']\n",
    "missing_appet = (appet.isnull().sum()/len(appet)) * 100\n",
    "print(\"missing appet: \", missing_appet, \"%\" )\n",
    "\n",
    "\n",
    "\n",
    "print(df_ckd_1['appet'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"appet\"].replace(np.nan, \"good\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ane\n",
    "\n",
    "#calculate the amount of missing data\n",
    "ane = df_ckd_1['ane']\n",
    "ane.describe()\n",
    "missing_ane = (ane.isnull().sum()/len(ane)) * 100\n",
    "print(\"missing ane: \", missing_ane, \"%\" )\n",
    "\n",
    "#######################################\n",
    "\n",
    "print(df_ckd_1['ane'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"ane\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe\n",
    "\n",
    "#calculate the amount of missing data\n",
    "pe = df_ckd_1['pe']\n",
    "missing_pe = (pe.isnull().sum()/len(pe)) * 100\n",
    "print(\"missing pe: \", missing_pe, \"%\" )\n",
    "\n",
    "\n",
    "\n",
    "print(df_ckd_1['pe'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"pe\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cad\n",
    "\n",
    "#calculate the amount of missing data\n",
    "cad = df_ckd_1['cad']\n",
    "missing_cad = (cad.isnull().sum()/len(cad)) * 100\n",
    "print(\"missing cad: \", missing_cad, \"%\" )\n",
    "\n",
    "\n",
    "df_ckd_1[\"cad\"].replace(\"\\tno\", \"no\", inplace=True)\n",
    "\n",
    "print(df_ckd_1['cad'].value_counts())\n",
    "#Replace the \"cad\" with the most frequent density\n",
    "df_ckd_1[\"cad\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#htn\n",
    "\n",
    "#calculate the amount of missing data\n",
    "htn = df_ckd_1['htn']\n",
    "missing_htn = (htn.isnull().sum()/len(htn)) * 100\n",
    "print(\"missing htn: \", missing_htn, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['htn'].value_counts())\n",
    "\n",
    "#Replace the \"htn\" with the most frequent density\n",
    "df_ckd_1[\"htn\"].replace(np.nan, \"no\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm\n",
    "\n",
    "#calculate the amount of missing data\n",
    "dm = df_ckd_1['dm']\n",
    "missing_dm = (dm.isnull().sum()/len(dm)) * 100\n",
    "print(\"missing dm: \", missing_dm, \"%\" )\n",
    "\n",
    "\n",
    "#Fixing the space issue in our dataframe\n",
    "df_ckd_1[\"dm\"].replace(\"\\tno\", \"no\", inplace=True)\n",
    "df_ckd_1[\"dm\"].replace(\"\\tyes\", \"yes\", inplace=True)\n",
    "\n",
    "print(df_ckd_1['dm'].value_counts())\n",
    "#Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"dm\"].replace(np.nan, \"no\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcc\n",
    "#calculate the amount of missing data\n",
    "pcc = df_ckd_1['pcc']\n",
    "missing_pcc = (pcc.isnull().sum()/len(pcc)) * 100\n",
    "print(\"missing pcc: \", missing_pcc, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['pcc'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"pcc\"].replace(np.nan, \"notpresent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ba\n",
    "ba = df_ckd_1['ba']\n",
    "missing_ba = (ba.isnull().sum()/len(ba)) * 100\n",
    "print(\"missing ba: \", missing_ba, \"%\" )\n",
    "\n",
    "print(df_ckd_1['ba'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"ba\"].replace(np.nan, \"notpresent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "\n",
    "age = df_ckd_1['age']\n",
    "print(age.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_age = (age.isnull().sum()/len(age)) * 100\n",
    "number_of_missing_data=age.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "print(\"missing age: \", missing_age, \"%\" )\n",
    "sns.distplot(age)\n",
    "\n",
    "# Adding in the missing values with average for age\n",
    "\n",
    "avg_age=df_ckd_1[\"age\"].mean(axis=0)\n",
    "print(\"Average of age\", avg_age)\n",
    "df_ckd_1[\"age\"].replace(np.nan, avg_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc\n",
    "sc = df_ckd_1['sc']\n",
    "print(sc.describe()) #calculate the amount of missing data\n",
    "missing_sc = (sc.isnull().sum()/len(sc)) * 100\n",
    "print(\"missing sc: \", missing_sc, \"%\" )\n",
    "sns.distplot(sc)\n",
    "\n",
    "avg_sc=df_ckd_1[\"sc\"].mean(axis=0)\n",
    "print(\"Average of sc\", avg_sc)\n",
    "df_ckd_1[\"sc\"].replace(np.nan, avg_sc, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bp\n",
    "# it is a continous variable. May look categorical\n",
    "bp = df_ckd_1['bp']\n",
    "print(bp.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_bp = (bp.isnull().sum()/len(bp)) * 100\n",
    "print(\"missing bp: \", missing_bp, \"%\" )\n",
    "sns.distplot(bp)\n",
    "\n",
    "# Updating the missing values\n",
    "avg_bp=df_ckd_1[\"bp\"].mean(axis=0)\n",
    "print(\"Average of bp\", avg_bp)\n",
    "df_ckd_1[\"bp\"].replace(np.nan, avg_bp, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bu \n",
    "bu = df_ckd_1['bu']\n",
    "print(bu.describe()) #calculate the amount of missing data\n",
    "missing_bu = (bu.isnull().sum()/len(bu)) * 100\n",
    "print(\"missing bu: \", missing_bu, \"%\" )\n",
    "sns.distplot(bu)\n",
    "\n",
    "# Updating the bu\n",
    "avg_bu=df_ckd_1[\"bu\"].mean(axis=0)\n",
    "print(\"Average of bu\", avg_bu)\n",
    "df_ckd_1[\"bu\"].replace(np.nan, avg_bu, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# al\n",
    "al = df_ckd_1['al']\n",
    "print(al.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_al = (al.isnull().sum()/len(al)) * 100\n",
    "print(\"missing rbc: \", missing_al, \"%\" )\n",
    "al_group = df_ckd.groupby('su').id.agg(['count'])\n",
    "print(al_group)\n",
    "\n",
    "# Replace the \"al\" with the most frequent density\n",
    "df_ckd_1[\"al\"].replace(np.nan, \"0.0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg\n",
    "# Even though it should be continous. it is categorical\n",
    "sg = df_ckd_1['sg']\n",
    "print(sg.describe()) #calculate the amount of missing data\n",
    "missing_sg = (sg.isnull().sum()/len(sg)) * 100\n",
    "print(\"missing sg: \", missing_sg, \"%\" )\n",
    "print(df_ckd_1['sg'].value_counts())\n",
    "\n",
    "# Replace the \"sg\" with the most frequent density\n",
    "df_ckd_1[\"sg\"].replace(np.nan, \"1.020\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# su variable \n",
    "# It is a categorical variable\n",
    "# Convert that into an object\n",
    "df_ckd_1[\"su\"].astype(object)\n",
    "\n",
    "su = df_ckd_1['su']\n",
    "print(su.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_su = (su.isnull().sum()/len(su)) * 100\n",
    "print(\"missing su: \", missing_al, \"%\" )\n",
    "\n",
    "#Finding the most frequent\n",
    "su_group = df_ckd.groupby('su').id.agg(['count'])\n",
    "print(su_group)\n",
    "\n",
    "\n",
    "# Replace the \"al\" with the most frequent density\n",
    "df_ckd_1[\"su\"].replace(np.nan, \"0.0\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hemo\n",
    "hemo = df_ckd_1['hemo']\n",
    "print(hemo.describe()) #calculate the amount of missing data\n",
    "missing_hemo = (hemo.isnull().sum()/len(hemo)) * 100\n",
    "print(\"missing hemo: \", missing_hemo, \"%\" )\n",
    "number_of_missing_data=hemo.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "sns.distplot(hemo)\n",
    "\n",
    "avg_hemo=df_ckd_1[\"hemo\"].mean(axis=0)\n",
    "print(\"Average of hemo\", avg_hemo)\n",
    "df_ckd_1[\"hemo\"].replace(np.nan, avg_hemo, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgr\n",
    "bgr = df_ckd_1['bgr']\n",
    "print(bgr.describe())\n",
    "\n",
    "#calculate the amount of missing data\n",
    "missing_bgr = (bgr.isnull().sum()/len(bgr)) * 100\n",
    "print(\"missing ba: \", missing_bgr, \"%\" )\n",
    "sns.distplot(bgr)\n",
    "avg_bgr=df_ckd_1[\"bgr\"].mean(axis=0)\n",
    "print(\"Average of bgr\", avg_bgr)\n",
    "df_ckd_1[\"bgr\"].replace(np.nan, avg_bgr, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCV\n",
    "\n",
    "df_ckd_1[\"pcv\"].replace(\"\\t43\", \"43\", inplace=True)\n",
    "df_ckd_1[\"pcv\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_1['pcv'].value_counts()\n",
    "\n",
    "df_ckd_1[\"pcv\"] = df_ckd_1[\"pcv\"].astype(float)\n",
    "pcv = df_ckd_1['pcv']\n",
    "print(pcv.describe()) #calculate the amount of missing data\n",
    "missing_pcv = (pcv.isnull().sum()/len(pcv)) * 100\n",
    "print(\"missing pcv: \", missing_pcv, \"%\" )\n",
    "sns.distplot(pcv)\n",
    "\n",
    "\n",
    "avg_pcv=df_ckd_1[\"pcv\"].mean(axis=0)\n",
    "print(\"Average of pcv\", avg_pcv)\n",
    "# Replace the \"pcv\" with the mean \n",
    "df_ckd_1[\"pcv\"].replace(np.nan, avg_pcv, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc variable \n",
    "\n",
    "pc = df_ckd_1['pc']\n",
    "print(pc.describe())\n",
    "# calculate the amount of missing data\n",
    "missing_pc = (pc.isnull().sum()/len(pc)) * 100\n",
    "print(\"missing pc: \", missing_al, \"%\" )\n",
    "\n",
    "\n",
    "print(df_ckd_1['pc'].value_counts())\n",
    "# Replace the \"pc\" with the most frequent density\n",
    "df_ckd_1[\"pc\"].replace(np.nan, \"normal\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Reviewing the current changes made ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()\n",
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################### Now handling the more complicated issue####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the sod\n",
    "\n",
    "* The current method is replaced with average.\n",
    "* correlated bp and cad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Data ready for the Linear Regression\n",
    "data = df_ckd_1[['id','bp','cad','sod']]\n",
    "# changing the cad\n",
    "data = pd.get_dummies(data, columns = ['cad'])\n",
    "del data['cad_no']\n",
    "missing_data = data['sod'].isnull()\n",
    "missing_data_tip = pd.DataFrame(data[['bp','cad_yes']][missing_data])\n",
    "data_nomissing = data.dropna()\n",
    "x = data_nomissing[['bp','cad_yes']]\n",
    "y = data_nomissing['sod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "# Descriptive statistic for evaluating the model\n",
    "yhat = lm.predict(X_test)\n",
    "SS_R = sum((yhat-np.mean(y_test))**2)       \n",
    "SS_Total = sum((y_test-np.mean(y_test))**2) \n",
    "r_squared = (float(SS_R))/SS_Total\n",
    "mse = mean_squared_error(y_test, yhat)\n",
    "\n",
    "print(\"r_sqaured: \",round(r_squared,2))\n",
    "print(\"mse: \", round(mse,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing on to the orginal dataframe\n",
    "data_pred = lm.predict(missing_data_tip)\n",
    "missing_data_tip['data_pred'] = data_pred\n",
    "data[\"data_pred\"] = missing_data_tip[\"data_pred\"]\n",
    "data[\"sod\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"data_pred\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"sod\"] = data[\"sod\"]+data[\"data_pred\"]\n",
    "df_ckd_1[\"sod\"]=data[\"sod\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing with the mean average (NOT USED ANYMORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sod \n",
    "\n",
    "# sod = df_ckd_1['sod']\n",
    "# print(sod.describe()) #calculate the amount of missing data\n",
    "# missing_sod = (sod.isnull().sum()/len(sod)) * 100\n",
    "# print(\"missing sod: \", missing_sod, \"%\" )\n",
    "# sns.distplot(sod)\n",
    "\n",
    "# # Changing sod\n",
    "# avg_sod=df_ckd_1[\"sod\"].mean(axis=0)\n",
    "# print(\"Average of sod\", avg_sod)\n",
    "# df_ckd_1[\"sod\"].replace(np.nan, avg_sod, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WC\n",
    "\n",
    "* We found an article that htn has a high correlation with the wc. \n",
    "* htn: yes has an average of 8094\n",
    "* htn: no has an average 8988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc\n",
    "\n",
    "df_ckd_1[\"wc\"].describe\n",
    "wc = df_ckd_1['wc']\n",
    "missing_wc = (wc.isnull().sum()/len(wc)) * 100\n",
    "print(\"missing wc: \", missing_wc, \"%\" )\n",
    "wc.describe()\n",
    "\n",
    "# Correct the wc from missing data and into float\n",
    "df_ckd_1[\"wc\"].replace(\"\\t?\",np.nan, inplace=True)\n",
    "df_ckd_1[['wc']]=df_ckd_1[['wc']].astype(float)\n",
    "wc.describe()\n",
    "\n",
    "# Need to replace the missing values for wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.groupby('htn').wc.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above results, we need to replace 8094 for htn is no , and replace 8988 for htn is yes\n",
    "for i in range(len(df_ckd_1['wc'])):\n",
    "    if pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]) == True and df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('htn')] == 'yes':\n",
    "        df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]= 8988\n",
    "        print(i)\n",
    "    elif pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]) == True and df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('htn')] == 'no':\n",
    "        df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('wc')]= 8094\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dealing with RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC\n",
    "corrMatrix = df_ckd_1.corr()\n",
    "print (corrMatrix)\n",
    "\n",
    "# From the correlatoin matrix. PCV and RC are highly correlated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Data ready for the Linear Regression\n",
    "data = df_ckd_1[['id','rc','hemo']]\n",
    "\n",
    "missing_data = data['rc'].isnull()\n",
    "missing_data_tip = pd.DataFrame(data[['hemo']][missing_data])\n",
    "data_nomissing = data.dropna()\n",
    "x = data_nomissing[['hemo']]\n",
    "y = data_nomissing['rc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "# Descriptive statistic for evaluating the model\n",
    "yhat = lm.predict(X_test)\n",
    "SS_R = sum((yhat-np.mean(y_test))**2)       \n",
    "SS_Total = sum((y_test-np.mean(y_test))**2) \n",
    "r_squared = (float(SS_R))/SS_Total\n",
    "mse = mean_squared_error(y_test, yhat)\n",
    "\n",
    "print(\"r_sqaured: \",round(r_squared,2))\n",
    "print(\"mse: \", round(mse,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing on to the orginal dataframe\n",
    "data_pred = lm.predict(missing_data_tip)\n",
    "missing_data_tip['data_pred'] = data_pred\n",
    "data[\"data_pred\"] = missing_data_tip[\"data_pred\"]\n",
    "data[\"rc\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"data_pred\"].replace(np.nan, 0, inplace=True)\n",
    "data[\"rc\"] = data[\"rc\"]+data[\"data_pred\"]\n",
    "df_ckd_1[\"rc\"]=data[\"rc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code below no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC \n",
    "\n",
    "# df_ckd_1[\"rc\"].astype(float)\n",
    "\n",
    "# #calculate the amount of missing data\n",
    "# rc = df_ckd_1['rc']\n",
    "# missing_rc = (rc.isnull().sum()/len(rc)) * 100\n",
    "# print(\"missing rc: \", missing_rc, \"%\" )\n",
    "# print(df_ckd_1['rc'].describe())\n",
    "\n",
    "# # Replace it with mean \n",
    "# avg_rc=df_ckd_1[\"rc\"].mean(axis=0)\n",
    "# print(\"Average of rc\", avg_rc)\n",
    "# df_ckd_1[\"rc\"].replace(np.nan, avg_rc, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focus on RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBC\n",
    "\n",
    "#calculate the amount of missing data\n",
    "rbc = df_ckd_1['rbc']\n",
    "missing_rbc = (rbc.isnull().sum()/len(rbc)) * 100\n",
    "print(\"missing rbc: \", missing_rbc, \"%\" )\n",
    "\n",
    "number_of_missing_data=rbc.isnull().sum()\n",
    "print(\"Number of missing: \", number_of_missing_data)\n",
    "\n",
    "print(df_ckd_1['rbc'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_ckd_1)):\n",
    "    if pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('rbc')]) == True and df_ckd_1.iloc[i]['rc'] > 3.92 and df_ckd_1.iloc[i]['rc'] < 5.65:\n",
    "        df_ckd_1.iloc[i, df_ckd_1.columns.get_loc('rbc')] = 'normal'\n",
    "        \n",
    "    elif pd.isnull(df_ckd_1.iloc[i,df_ckd_1.columns.get_loc('rbc')]) == True:\n",
    "        df_ckd_1.iloc[i, df_ckd_1.columns.get_loc('rbc')] = 'abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the update_ckd file\n",
    "df_ckd_1.to_csv(\"df_ckd_1.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers for the continous variable\n",
    "It deleted too many variables. So this section was not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 = df_ckd_1.quantile(0.25)\n",
    "# Q3 = df_ckd_1.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ckd_2 = df_ckd_1[~((df_ckd_1 < (Q1 - 1.5 * IQR)) |(df_ckd_1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# df_ckd_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ckd_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Code the categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the sg\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['sg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the al\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['al'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the su\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['su'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the rbc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['rbc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pcc\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the ba\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['ba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the htn\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['htn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the dm\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['dm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the cad\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['cad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the appet\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['appet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the pe\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['pe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the ane\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['ane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the classification\n",
    "df_ckd_1 = pd.get_dummies(df_ckd_1, columns = ['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping unused column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing percent_missing\n",
    "del df_ckd_1['percent_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the categorical n-1 variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['sg_1.005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['al_0.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['su_0.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['rbc_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pc_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pcc_notpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['ba_notpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['htn_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['dm_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['cad_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['appet_poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['pe_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['ane_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['classification_notckd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ckd_1['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.to_csv(\"df_ckd_1.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "* Rename the variable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.rename(columns={'rbc_abnormal':'rbc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'ba_present':'ba'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pc_abnormal':'pc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pcc_present':'pcc'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'htn_yes':'htn'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'dm_yes':'dm'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'cad_yes':'cad'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'appet_good':'appet'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'pe_yes':'pe'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'ane_yes':'ane'}, inplace=True)\n",
    "df_ckd_1.rename(columns={'classification_ckd':'classification'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################\n",
    "* Complete data wrangling\n",
    "* Next, set training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for predictors\n",
    "df_ckd_predictors = df_ckd_1.loc[:, df_ckd_1.columns != 'classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for response\n",
    "df_ckd_response = df_ckd_1['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% trainning and 40% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ckd_predictors, df_ckd_response, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"X_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"y_train.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(\"y_test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################\n",
    "Model Comparison ######################################\n",
    "1. Logistic: Taylor, Rachel \n",
    "2. Nearest Neighbor: Peter, Juilanne \n",
    "3. Random Forest: Gabby, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the performance of model\n",
    "1) Beat the naive rule \n",
    "2) validation hitrate and trainning hitrate within 3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_count = y_test.sum()\n",
    "naive_length = len(y_test)\n",
    "\n",
    "Naive_rule = naive_count/naive_length * 100\n",
    "print(\"Naive Rule: \", round(Naive_rule, 2) ,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for measuring our model performance used in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate: just pass in the confusion matrix of either validation data or training data\n",
    "def calcHR(conf):\n",
    "    hey=conf[0,0]\n",
    "    ho=conf[1,1]\n",
    "    result=(hey+ho)/conf.sum()*100\n",
    "    return result\n",
    "# False Positive: input confusion matrix to calculate the false positives in the data set. return a %\n",
    "def calcFP(conf):\n",
    "    result=(conf[0,1].astype('float'))/(conf[0,1].astype('float')+conf[1,1].astype('float')) * 100\n",
    "    return result\n",
    "# False Negative: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcFN(conf):\n",
    "    result=(conf[1,0].astype('float'))/(conf[0,0].astype('float')+conf[1,0].astype('float')) * 100\n",
    "    return result\n",
    "# Sensitivity: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcSens(conf):\n",
    "    result=(conf[1,1].astype('float'))/(conf[1,0].astype('float')+conf[1,1].astype('float')) * 100\n",
    "    return result\n",
    "# Specificity: input confusion matrix to calculate the false positives in the data set. returns a %\n",
    "def calcSpec(conf):\n",
    "    result=(conf[0,0].astype('float'))/(conf[0,0].astype('float')+conf[0,1].astype('float')) * 100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do List\n",
    "\n",
    "\n",
    "1) ROC Curve\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this website as reference: \n",
    "https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K = 15\n",
    "# Classifier with Euclidean distance for determining the proximity between neighboring points\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_validate = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "confValid = confusion_matrix(y_test, y_pred_validate)\n",
    "confTrain = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class) or the Naive rule\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = knn.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('Naive Rule: ROC AUC=%.3f' % (ns_auc))\n",
    "print('NNA: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Nearest Neighbor')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying different nearest neighbor number from 1 to 9\n",
    "\n",
    "\n",
    "Writing the for loop through different k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 20,2)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "test_hitrate = np.empty(len(dep))\n",
    "test_false_pos = np.empty(len(dep))\n",
    "test_false_neg = np.empty(len(dep))\n",
    "test_sens = np.empty(len(dep))\n",
    "test_spec = np.empty(len(dep))\n",
    "test_one_minus_spec = np.empty(len(dep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "    \n",
    "\n",
    "# Generate plot\n",
    "plt.title('Different k')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Different k values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create a list\n",
    "def createList(r1, r2): \n",
    "    return [item for item in range(r1, r2+1)] \n",
    "      \n",
    "# testing it out:\n",
    "r1, r2 = -1, 1\n",
    "print(createList(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "NNA_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "NNA_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "NNA_hitrate_summary['Check_for_vaild'] = NNA_hitrate_summary['train_accuracy']-NNA_hitrate_summary['test_accuracy']\n",
    "NNA_hitrate_summary['k_value']=np.arange(1, 20,2)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = NNA_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "NNA_hitrate_summary = NNA_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################Logistic Regressoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to look into forward selection \n",
    "\n",
    "\n",
    "Need to look into backward selection \n",
    "\n",
    "Need to get odds-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_validate1=logreg.predict(X_test)\n",
    "y_pred_train1 = logreg.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid1 = confusion_matrix(y_test, y_pred_validate1)\n",
    "confTrain1 = confusion_matrix(y_train, y_pred_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain1),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid1),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid1),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid1),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid1),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Model's ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Logistic: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.exp(logreg.intercept_).astype('float')\n",
    "model_odds = pd.DataFrame(df_ckd_predictors.columns)\n",
    "model_odds['odds-ratio'] = pd.DataFrame(np.exp(logreg.coef_ * 1 + logreg.intercept_)).transpose()/base\n",
    "model_odds.rename(columns={0:'predictors'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, bootstrap=True)\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "y_pred_validate2=model.predict(X_test)\n",
    "y_pred_train2 = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns),\n",
    "'importance': model.feature_importances_}).\\\n",
    "sort_values('importance', ascending = False)\n",
    "fi.head()\n",
    "\n",
    "def plot_fi(fi):\n",
    "    return fi.plot('feature','importance','barh', figsize=(12,7), legend=False)\n",
    "\n",
    "# fi[:15] label the number of variable cutoff\n",
    "plot_fi(fi[:15]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid2 = confusion_matrix(y_test, y_pred_validate2)\n",
    "confTrain2 = confusion_matrix(y_train, y_pred_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confValid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain2),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid2),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid2),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid2),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid2),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = model.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Random Forest: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the Randomforest\n",
    "Varying the branches of the randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "\n",
    "for i, k in enumerate(dep):\n",
    "    clf = RandomForestClassifier(max_depth=k)\n",
    "\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    train_accuracy[i] = clf.score(X_train, y_train)\n",
    "\n",
    "    test_accuracy[i] = clf.score(X_test, y_test)\n",
    "\n",
    "plt.title('clf: Varying depth of forest branch')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "RF_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "RF_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "RF_hitrate_summary['Check_for_vaild'] = RF_hitrate_summary['train_accuracy']-RF_hitrate_summary['test_accuracy']\n",
    "RF_hitrate_summary['Depth of tree']=createList(1, 19)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = RF_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "RF_hitrate_summary = RF_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hitrate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################\n",
    "Ideas for some improvement on NNA and Logistic\n",
    "* Use the important variable identified from Random Forest\n",
    "* hemo, pcv, sc, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckd_response = df_ckd_1['classification']\n",
    "features=['hemo','pcv','sc','rc']\n",
    "df_ckd_predictors = df_ckd_1[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ckd_predictors, df_ckd_response, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 20, 2)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "test_hitrate = np.empty(len(dep))\n",
    "test_false_pos = np.empty(len(dep))\n",
    "test_false_neg = np.empty(len(dep))\n",
    "test_sens = np.empty(len(dep))\n",
    "test_spec = np.empty(len(dep))\n",
    "test_one_minus_spec = np.empty(len(dep))\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plt.title('Different k')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Different k values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary = pd.DataFrame(train_accuracy)\n",
    "NNA_hitrate_summary.rename(columns={0:'train_accuracy'},inplace = True)\n",
    "NNA_hitrate_summary['test_accuracy'] =test_accuracy\n",
    "NNA_hitrate_summary['Check_for_vaild'] = NNA_hitrate_summary['train_accuracy']-NNA_hitrate_summary['test_accuracy']\n",
    "NNA_hitrate_summary['k_value']=np.arange(1, 20, 2)\n",
    "\n",
    "# Moving the last column to the front\n",
    "cols = NNA_hitrate_summary.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "NNA_hitrate_summary = NNA_hitrate_summary[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNA_hitrate_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K = 3\n",
    "# Classifier with Euclidean distance for determining the proximity between neighboring points\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_validate = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "confValid = confusion_matrix(y_test, y_pred_validate)\n",
    "confTrain = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class) or the Naive rule\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = knn.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('Naive Rule: ROC AUC=%.3f' % (ns_auc))\n",
    "print('NNA: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Nearest Neighbor')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_validate1=logreg.predict(X_test)\n",
    "y_pred_train1 = logreg.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion_matrix\n",
    "confValid4 = confusion_matrix(y_test, y_pred_validate1)\n",
    "confTrain4 = confusion_matrix(y_train, y_pred_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Model Metrics\n",
    "print(\"\\033[1m Naive Rule: \\033[0m\", str(round(Naive_rule,2)) + \"%\")\n",
    "print(\"\\033[1m Training hitrate: \\033[0m\" + str(round(calcHR(confTrain4),2)) + \"%\")\n",
    "print(\"\\033[1m Validation hitrate: \\033[0m\"+ str(round(calcHR(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m False Positives: \\033[0m\"+ str(round(calcFP(confValid4),2)) + \"%\")\n",
    "print(\"\\033[1m False Negatives: \\033[0m\"+ str(round(calcFN(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m Sensitivity: \\033[0m\"+ str(round(calcSens(confValid4),2)) + \"%\")\n",
    "print(\"\\033[1m Specificity: \\033[0m\"+ str(round(calcSpec(confValid4),2)) + \"%\")\n",
    "print(\" \")\n",
    "print(\"\\033[1m 1 - Specficity: \\033[0m\"+ str(round(100-calcSpec(confValid4),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs)\n",
    "rf_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "print('Naive Rule: AUROC = %.3f' % (r_auc))\n",
    "print('Logistic: AUROC = %.3f' % (rf_auc))\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle='--', label='Naive Rule')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('1 - Specficity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend() #\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.exp(logreg.intercept_).astype('float')\n",
    "model_odds = pd.DataFrame(df_ckd_predictors.columns)\n",
    "model_odds['odds-ratio'] = pd.DataFrame(np.exp(logreg.coef_ * 1 + logreg.intercept_)).transpose()/base\n",
    "model_odds.rename(columns={0:'predictors'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_odds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
